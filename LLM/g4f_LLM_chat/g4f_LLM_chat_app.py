# CHAT WITH LLM USING THE G4F LIBRARY
#
# conda activate extras2
# pip install -U g4f nodriver platformdirs curl_cffi browser_cookie3

'''
2025-07-13 - cÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:

/model 1 - gpt-4
/model 3 - gpt-4o-mini
/model 11 - gpt-4.1-mini
/model 12 - gpt-4.1-nano
/model 10 - gpt-4.1
/model 36 - phi-4
/model 42 - gemini-1.5-flash
/model 43 - gemini-1.5-pro
/model 56 - gemma-3-27b
/model 57 - gemma-3n-e4b
/model 58 - blackboxai
/model 59 - command-r
/model 62 - command-a
/model 66 - qwen-2-vl-72b
/model 70 - qwen-2.5-coder-32b
/model 72 - qwen-2.5-max
/model 73 - qwen-2.5-vl-72b (Ñ€ĞµĞ¶ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚, Ñ€Ğ°Ğ·Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒÑÑ)
/model 74 - qwen-3-235b
/model 81 - qwq-32b
/model 93 - deepseek-r1-0528
/model 94 - deepseek-r1-0528-turbo
/model 98 - grok-3-mini
/model 102 - sonar-reasoning
/model 103 - sonar-reasoning-pro
/model 104 - r1-1776 (Ñ€Ğ¸Ğ·Ğ¾Ğ½Ğ¸Ğ½Ğ³)
/model 105 - nemotron-70b (Ğ”Ğ¾Ğ»Ğ³Ğ¾)
/model 112 - evil
'''

import g4f
import time
import asyncio
import os
from datetime import datetime
from g4f import Provider
from g4f.models import Model, ModelUtils


# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ cookies, ĞµÑĞ»Ğ¸ ĞµÑ‘ Ğ½ĞµÑ‚
def ensure_cookie_dir():
    appdata = os.getenv('APPDATA')
    if appdata is None:
        appdata = os.path.expanduser('~')
    cookie_dir = os.path.join(appdata, 'g4f', 'cookies')
    if not os.path.exists(cookie_dir):
        os.makedirs(cookie_dir, exist_ok=True)
        print(f"ğŸ“ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¿Ğ°Ğ¿ĞºĞ° Ğ´Ğ»Ñ cookies: {cookie_dir}")
    return cookie_dir


# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²
def get_available_text_providers():
    excluded_keywords = [
        'vision', 'image', 'img', 'dall-e', 'clip', 'whisper', 'flux',
        'tts', 'speech', 'audio', 'ocr', 'stable-diffusion', 'sd'
    ]

    providers = []
    print("ğŸ” Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€ÑƒÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹...")

    for attr in dir(Provider):
        if attr.startswith('__') or attr == 'BaseProvider':
            continue

        provider = getattr(Provider, attr)
        if not hasattr(provider, 'model') and not hasattr(provider, 'supported_models'):
            continue

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹
        if hasattr(provider, 'model'):
            model_name = provider.model
            if any(kw in model_name.lower() for kw in excluded_keywords):
                continue
        elif hasattr(provider, 'supported_models'):
            if any(any(kw in m.lower() for kw in excluded_keywords) for m in provider.supported_models):
                continue

        providers.append(provider)

    print(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(providers)} Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²")
    return providers


# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
def get_available_text_models():
    excluded_keywords = [
        'vision', 'image', 'img', 'dall-e', 'clip', 'whisper', 'flux',
        'tts', 'speech', 'audio', 'ocr', 'stable-diffusion', 'sd'
    ]

    print("ğŸ” Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€ÑƒÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
    models = [
        model for model in ModelUtils.convert
        if not any(keyword in model.lower() for keyword in excluded_keywords)
    ]
    print(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(models)} Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹")
    return models


# ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
async def get_streaming_response(model, messages):
    try:
        print("ğŸ”„ ĞŸÑ€Ğ¾Ğ±ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€...")
        start_time = time.time()

        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ¼
        try:
            response_stream = g4f.ChatCompletion.create_async(
                model=model,
                messages=messages,
                stream=True
            )
            print("âœ… ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»!")
            return response_stream, start_time, "Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹"
        except Exception as stream_error:
            print(f"âŒ ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ: {str(stream_error)}")
            # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼
            response_coro = g4f.ChatCompletion.create_async(
                model=model,
                messages=messages,
            )
            response = await response_coro
            print("âœ… ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼)!")
            return response, start_time, "Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹"

    except Exception as e:
        print(f"âŒ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»: {str(e)}")
        print("ğŸ”„ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ±Ğ¾Ñ€ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²...")

        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹, Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹
        providers = get_available_text_providers()
        print(f"ğŸ“‹ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(providers)} Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²")

        for i, provider in enumerate(providers, 1):
            try:
                print(f"ğŸ”„ [{i}/{len(providers)}] ĞŸÑ€Ğ¾Ğ±ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€: {provider.__name__}")
                start_time = time.time()

                # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼
                try:
                    response_stream = g4f.ChatCompletion.create_async(
                        model=model,
                        messages=messages,
                        provider=provider,
                        stream=True
                    )
                    print(f"âœ… ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ {provider.__name__} ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» (Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼)!")
                    return response_stream, start_time, provider.__name__
                except Exception as stream_error:
                    print(f"âš ï¸ ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ, Ğ¿Ñ€Ğ¾Ğ±ÑƒÑ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹...")
                    response_coro = g4f.ChatCompletion.create_async(
                        model=model,
                        messages=messages,
                        provider=provider
                    )
                    response = await response_coro
                    print(f"âœ… ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ {provider.__name__} ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼)!")
                    return response, start_time, provider.__name__

            except Exception as provider_error:
                print(f"âŒ ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ {provider.__name__} Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»: {str(provider_error)}")
                continue

        print("âŒ Ğ’ÑĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸")
        return f"Ğ’ÑĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸. ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)}", 0, "Ğ¾ÑˆĞ¸Ğ±ĞºĞ°"


# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¸ÑĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
def show_model_list(models, current_model=None):
    print("\nğŸ“š Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:")
    for i, model in enumerate(models, 1):
        prefix = "â¤ " if current_model and model == current_model else "  "
        print(f"{prefix}{i}. {model}")


# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ†Ğ¸ĞºĞ» Ñ‡Ğ°Ñ‚Ğ°
async def chat_with_model(selected_model, text_models, messages=None):
    if messages is None:
        messages = []

    print(f"\nğŸš€ ĞĞ°Ñ‡Ğ°Ñ‚ Ñ‡Ğ°Ñ‚ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ: {selected_model}")
    print(
        "ğŸ’¬ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ (ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹: /list - Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, /model N - ÑĞ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, /clear - Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ, /exit - Ğ²Ñ‹Ñ…Ğ¾Ğ´)")
    print("=" * 50)

    session_start_time = datetime.now()
    print(f"â± Ğ’Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞµÑÑĞ¸Ğ¸: {session_start_time.strftime('%H:%M:%S')}")

    while True:
        try:
            user_input = input("\nĞ’Ñ‹: ").strip()

            # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´
            if user_input.lower() == '/exit':
                print("\nĞ¡ĞµÑÑĞ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°.")
                session_duration = datetime.now() - session_start_time
                print(f"â± ĞĞ±Ñ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞµÑÑĞ¸Ğ¸: {session_duration}")
                return messages

            if user_input.lower() == '/list':
                show_model_list(text_models, selected_model)
                continue

            if user_input.lower() == '/clear':
                messages = []
                print("\nğŸ”„ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ°")
                continue

            if user_input.lower().startswith('/model'):
                parts = user_input.split()
                if len(parts) > 1:
                    try:
                        new_index = int(parts[1]) - 1
                        if 0 <= new_index < len(text_models):
                            new_model = text_models[new_index]
                            if new_model == selected_model:
                                print(f"â„¹ï¸ Ğ’Ñ‹ ÑƒĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ {selected_model}")
                            else:
                                print(f"\nğŸ”„ Ğ¡Ğ¼ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {selected_model} â†’ {new_model}")
                                selected_model = new_model
                        else:
                            print(f"âŒ ĞĞµĞ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ñ‚ 1 Ğ´Ğ¾ {len(text_models)}")
                    except ValueError:
                        print("âŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ: /model ĞĞĞœĞ•Ğ ")
                else:
                    show_model_list(text_models, selected_model)
                continue

            # ĞĞ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
            messages.append({"role": "user", "content": user_input})

            print(f"\nâŒ› {selected_model} Ğ´ÑƒĞ¼Ğ°ĞµÑ‚...")
            result = await get_streaming_response(selected_model, messages)

            if len(result) == 3:
                response, response_start_time, provider_name = result

                if isinstance(response, str):
                    # ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ (Ğ½Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹)
                    response_time = time.time() - response_start_time
                    messages.append({"role": "assistant", "content": response})
                    print(f"\n{selected_model}: {response}")
                    print(f"â± Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°: {response_time:.2f} ÑĞµĞºÑƒĞ½Ğ´")
                else:
                    # ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼
                    print(f"\n{selected_model} ({provider_name}): ", end="", flush=True)
                    full_response = ""

                    try:
                        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚
                        async for chunk in response:
                            if chunk:
                                print(chunk, end="", flush=True)
                                full_response += str(chunk)
                        print()  # ĞĞ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ° Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ

                        response_time = time.time() - response_start_time
                        messages.append({"role": "assistant", "content": full_response})
                        print(f"â± Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°: {response_time:.2f} ÑĞµĞºÑƒĞ½Ğ´")
                    except Exception as stream_error:
                        print(f"\nâŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°: {str(stream_error)}")
                        if full_response:
                            messages.append({"role": "assistant", "content": full_response})
            else:
                # ĞÑˆĞ¸Ğ±ĞºĞ°
                error_message = result
                print(f"\nâŒ {error_message}")

            print("-" * 50)

        except KeyboardInterrupt:
            print("\n\nĞ¡ĞµÑÑĞ¸Ñ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼.")
            return messages
        except Exception as e:
            print(f"\nâš ï¸ ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {str(e)}")
            print("ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ· Ğ¸Ğ»Ğ¸ ÑĞ¼ĞµĞ½Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (/model)")


# Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
async def main():
    print("ğŸš€ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹...")

    # Ğ£Ğ±ĞµĞ´Ğ¸Ğ¼ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ°Ğ¿ĞºĞ° Ğ´Ğ»Ñ cookies ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
    print("ğŸ“ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑÑ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ cookies...")
    ensure_cookie_dir()

    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
    print("ğŸ” Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹...")
    text_models = get_available_text_models()

    if not text_models:
        print("âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.")
        return

    print("âœ… Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")

    # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    show_model_list(text_models)

    current_messages = []
    selected_model = None

    while True:
        try:
            if not selected_model:
                choice = int(input("\nğŸ‘‰ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ°: "))
                if 1 <= choice <= len(text_models):
                    selected_model = text_models[choice - 1]
                    print(f"ğŸ”„ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {selected_model}")
                else:
                    print(f"âŒ ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ñ‚ 1 Ğ´Ğ¾ {len(text_models)}")
                    continue

            # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ñ‡Ğ°Ñ‚ Ñ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ
            current_messages = await chat_with_model(
                selected_model,
                text_models,
                current_messages
            )

            # ĞŸĞ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ‡Ğ°Ñ‚Ğ° ÑĞ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµĞ¼ Ğ¾ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…
            print("\nĞ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ:")
            print("1. ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ñ‡Ğ°Ñ‚ Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ")
            print("2. Ğ’Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")
            print("3. Ğ’Ñ‹Ğ¹Ñ‚Ğ¸")

            action = input("Ğ’Ğ°Ñˆ Ğ²Ñ‹Ğ±Ğ¾Ñ€ (1-3): ").strip()

            if action == '3':
                print("\nĞ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ!")
                break
            elif action == '2':
                selected_model = None
                show_model_list(text_models)
            elif action != '1':
                print("â© ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ...")

        except ValueError:
            print("âŒ ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ½Ğ¾Ğ¼ĞµÑ€")
        except KeyboardInterrupt:
            print("\n\nĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°.")
            break


if __name__ == "__main__":
    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ´Ğ»Ñ Windows
    if os.name == 'nt':
        import asyncio

        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    asyncio.run(main())